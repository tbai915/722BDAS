{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import findspark\n",
    "findspark.init('C:\\spark')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession,Row\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import DecisionTreeRegressor,LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import IsotonicRegression\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName('basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_average_vehicle(file_dir):\n",
    "    average_vehicle_df = pd.read_excel(file_dir, sheet_name=\"2.1, 2.2, 2.3,2.4\", header=2, nrows=19, usecols=\"A:AH\")\n",
    "    return average_vehicle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_light_fleet_age(file_dir):\n",
    "    light_fleet_age_df = pd.read_excel(file_dir, sheet_name=\"2.10\", header=1, nrows=7)\n",
    "    return light_fleet_age_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_co2_emission(file_dir):\n",
    "    co2_emission_df = pd.read_excel(file_dir, sheet_name=\"1.10\", header=2, nrows=17, usecols=\"A:E\")\n",
    "    return co2_emission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sort_array_index(array):\n",
    "    print(np.sort(array))\n",
    "    order = []\n",
    "    for element in np.sort(array):\n",
    "        for idx, pca_value in enumerate(array):\n",
    "            if element == pca_value:\n",
    "                order.append(idx)\n",
    "\n",
    "    print(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_2_4_1_imputation_co2_emission_df(co2_emission_df):\n",
    "    missing_values = [[2000] + [np.nan for i in range(4)], [2018] + [np.nan for i in range(4)]]\n",
    "\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imp.fit(co2_emission_df)\n",
    "    X = imp.transform(missing_values)\n",
    "\n",
    "    df = pd.DataFrame(X, columns=co2_emission_df.columns)\n",
    "    co2_emission_df = pd.concat([co2_emission_df, df])\n",
    "    co2_emission_df = co2_emission_df.sort_values(by=['Year'])\n",
    "    co2_emission_df.index = [x for x in range(len(co2_emission_df.index))]\n",
    "\n",
    "    return co2_emission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_normality(target, name):\n",
    "    mean,std = np.mean(target), np.std(target)\n",
    "    X = np.linspace(np.min(target), np.max(target), 1000)\n",
    "    pdf = stats.norm.pdf(X, mean, std)\n",
    "    plt.plot(X, pdf, label=\"PDF\")\n",
    "    plt.grid()\n",
    "    plt.title('Check Normal Distribution for %s' %name,fontsize=10)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_2_4_2_check_normality(number_fleets_df, light_fleet_age_df,co2_emission_df):\n",
    "    plot_normality(number_fleets_df['Total LPV new'], 'Total LPV new')\n",
    "    plot_normality(number_fleets_df[' Total LPV used'], 'Total LPV used')\n",
    "    plot_normality(number_fleets_df['Total LCV new'], 'Total LCV new')\n",
    "    plot_normality(number_fleets_df[' Total LCV used'], 'Total LCV used')\n",
    "\n",
    "    plot_normality(np.array(light_fleet_age_df.iloc[0][1:20].astype(int)), '0-4 age group')\n",
    "    plot_normality(np.array(light_fleet_age_df.iloc[1][1:20].astype(int)), '5-9 age group')\n",
    "    plot_normality(np.array(light_fleet_age_df.iloc[2][1:20].astype(int)), '10-14 age group')\n",
    "    plot_normality(np.array(light_fleet_age_df.iloc[3][1:20].astype(int)), '15-19 age group')\n",
    "    plot_normality(np.array(light_fleet_age_df.iloc[4][1:20].astype(int)), '20+ age group')\n",
    "\n",
    "\n",
    "    plot_normality(co2_emission_df['Light passenger'], 'Light passenger co2 emssion')\n",
    "    plot_normality(co2_emission_df['Light commercial'], 'Light commercial co2 emission')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_3_1_clean_light_age_distribution(light_fleet_age_df):\n",
    "    light_fleet_age_df = light_fleet_age_df.T\n",
    "    light_fleet_age_df.index = [x for x in range(len(light_fleet_age_df.index))]\n",
    "\n",
    "    numbers = pd.DataFrame(light_fleet_age_df[1:20])\n",
    "    numbers = numbers.drop(columns=[6])\n",
    "    numbers.columns = ['0-4 years', '5-9 years', '10-14 years', '15-19 years', '20+ years', 'Total']\n",
    "    numbers.index = [i for i in range(2000, 2019)]\n",
    "\n",
    "    percentages = pd.DataFrame(light_fleet_age_df[20:])\n",
    "    percentages = percentages.drop(columns=5)\n",
    "    percentages.columns = ['0-4 years percentage', '5-9 years  percentage',\n",
    "                           '10-14 years percentage', '15-19 years percentage',\n",
    "                           '20+ years percentage', '15+ years percentage']\n",
    "    percentages.index = [i for i in range(2000, 2019)]\n",
    "\n",
    "    new_age_distribution = pd.concat([numbers, percentages], axis=1, join='inner')\n",
    "    new_age_distribution.insert(0, 'Period', new_age_distribution.index)\n",
    "    new_age_distribution.index = [i for i in range(len(new_age_distribution.index))]\n",
    "\n",
    "    return new_age_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_3_3_construct_new_distribution_df(nums_columns, percentage_columns, number_fleets_df, new_age_distribution_df):\n",
    "\n",
    "    # new_table = {'Period':[i for i in range(2000,2019)]} for internal output\n",
    "    new_table = {}\n",
    "    for num_column in nums_columns:\n",
    "        for percenate_column in percentage_columns:\n",
    "            new_column = new_age_distribution_df[percenate_column] * number_fleets_df[num_column]\n",
    "            new_column_name = '%s of %s' % (percenate_column[:-11].strip(), num_column[6:].strip())\n",
    "            new_table[new_column_name] = new_column\n",
    "\n",
    "    new_age_distribution_df = pd.DataFrame(new_table)\n",
    "\n",
    "    return new_age_distribution_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_3_5_convert_object_to_int(data_df):\n",
    "    for column in data_df.columns:\n",
    "        if data_df.dtypes[column] != np.float64:\n",
    "            data_df[column] = data_df[column].astype(np.int64)\n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_4_1_LPV_cols(cleaned_data_df):\n",
    "    default_LPV_cols = ['Period', 'Total LPV new',' Total LPV used', 'Light passenger average age','0-4 years of LPV new', \n",
    "                    '5-9 years of LPV new', '10-14 years of LPV new', '15-19 years of LPV new', '20+ years of LPV new',\n",
    "                    '15+ years of LPV new', '0-4 years of LPV used', '5-9 years of LPV used', '10-14 years of LPV used',\n",
    "                    '15-19 years of LPV used', '20+ years of LPV used', '15+ years of LPV used', ]\n",
    "\n",
    "    LPV = cleaned_data_df[default_LPV_cols]\n",
    "    sparkdf = spark.createDataFrame(LPV)\n",
    "\n",
    "    assembler = VectorAssembler(inputCols= sparkdf.columns,outputCol=\"features\")\n",
    "    output = assembler.transform(sparkdf)\n",
    "    pca = PCA(k=16, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "    model = pca.fit(output)\n",
    "    print(model.explainedVariance)\n",
    "    \n",
    "    result = model.transform(output).select(\"pcaFeatures\")\n",
    "    result.show(truncate=False)\n",
    "    \n",
    "    LPV_PCA_list = result.head(2)\n",
    "\n",
    "    print(LPV_PCA_list[0][0])\n",
    "    get_sort_array_index(LPV_PCA_list[0][0])\n",
    "    print(LPV_PCA_list[1][0])         \n",
    "    get_sort_array_index(LPV_PCA_list[1][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_4_1_LCV_cols(cleaned_data_df):\n",
    "    default_LCV_cols = ['Period',  'Total LCV new', ' Total LCV used', 'Light commercial average age', '0-4 years of LCV new',\n",
    "                    '5-9 years of LCV new', '10-14 years of LCV new', '15-19 years of LCV new', '20+ years of LCV new',\n",
    "                    '0-4 years of LCV used', '5-9 years of LCV used', '10-14 years of LCV used','15-19 years of LCV used',\n",
    "                    '20+ years of LCV used', ]\n",
    "\n",
    "    LCV = cleaned_data_df[default_LCV_cols]\n",
    "    LCV_sparkdf = spark.createDataFrame(LCV)\n",
    "\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols= LCV_sparkdf.columns,\n",
    "        outputCol=\"features\")\n",
    "    LCV_output = assembler.transform(LCV_sparkdf)\n",
    "    \n",
    "    pca = PCA(k=14, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "    model = pca.fit(LCV_output)\n",
    "    print(model.explainedVariance)\n",
    "    \n",
    "    result = model.transform(LCV_output).select(\"pcaFeatures\")\n",
    "    result.show(truncate=False)\n",
    "    LCV_PCA_list = result.head(2)\n",
    "    \n",
    "    print(LCV_PCA_list[0][0])\n",
    "    get_sort_array_index(LCV_PCA_list[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_4_2_normalization(data_df,LPV_cols,LCV_cols):\n",
    "    LPV_df, LCV_df = data_df[LPV_cols],data_df[LCV_cols]\n",
    "    LPV_data,LCV_data = normalize( LPV_df, axis=1, norm='l2'),normalize(LCV_df, axis=1, norm='l2')\n",
    "    LPV_df, LCV_df = pd.DataFrame(LPV_data, columns= LPV_cols),pd.DataFrame(LCV_data, columns= LCV_cols)\n",
    "    return LPV_df, LCV_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_dataset(df, seed):\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "    assembler = VectorAssembler(inputCols= spark_df.columns[:-1],outputCol=\"features\")\n",
    "    spark_assembler_df = assembler.transform(spark_df)\n",
    "    \n",
    "    selected_data = spark_assembler_df.select('features',spark_df.columns[-1])\n",
    "    train_data,test_data = selected_data.randomSplit([0.7,0.3],seed=seed)\n",
    "    \n",
    "    noisy_df = spark.createDataFrame([[Vectors.dense(np.zeros(13)),0.0]])\n",
    "    train_data,test_data = train_data.union(noisy_df),test_data.union(noisy_df)\n",
    "    \n",
    "    return train_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_6_vertify_dt(train_data, test_data, labelcol):\n",
    "    dt = DecisionTreeRegressor(featuresCol='features', labelCol=labelcol)\n",
    "    dt_model =dt.fit(train_data)\n",
    "\n",
    "\n",
    "    predictions =dt_model.transform(test_data)\n",
    "    predictions.show()\n",
    "    evaluator = RegressionEvaluator(\n",
    "        labelCol=labelcol, predictionCol=\"prediction\", metricName=\"r2\")\n",
    "    r2 = evaluator.evaluate(predictions)\n",
    "    print(\"R2 on test data = %g\" % r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regressor(train_data, test_data, labelcol, reg_param):\n",
    "    \n",
    "    lr = LinearRegression(labelCol=labelcol, regParam=reg_param, elasticNetParam=0, loss='squaredError')\n",
    "#     lr = IsotonicRegression(labelCol=labelcol)\n",
    "\n",
    "    lrModel = lr.fit(train_data)\n",
    "\n",
    "    predictions = lrModel.transform(test_data)\n",
    "    test_results = lrModel.evaluate(test_data)\n",
    "    \n",
    "#     evaluator = RegressionEvaluator(labelCol=labelcol, predictionCol=\"prediction\", metricName=\"r2\")\n",
    "#     r2 = evaluator.evaluate(predictions)\n",
    "\n",
    "    return predictions, test_results.residuals, test_results.r2, lrModel\n",
    "#     return predictions, predictions.select('prediction'), r2,lrModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_7_1_assemble_dataset(df, seed):\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "    assembler = VectorAssembler(inputCols= spark_df.columns[:-1],outputCol=\"features\")\n",
    "    spark_assembler_df = assembler.transform(spark_df)\n",
    "    \n",
    "    selected_data = spark_assembler_df.select('features',spark_df.columns[-1])\n",
    "    train_data,test_data = selected_data.randomSplit([0.7,0.25],seed=seed)\n",
    "    \n",
    "    noisy_df = spark.createDataFrame([[Vectors.dense(np.zeros(13)),0.0] for _ in range(3)])\n",
    "\n",
    "    train_data,test_data = train_data.union(noisy_df),test_data.union(noisy_df)\n",
    "\n",
    "    \n",
    "    return train_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_7_1_objective_2_dataset(rows,columns):\n",
    "    spark_df = spark.createDataFrame(rows)\n",
    "    assembler = VectorAssembler(inputCols= spark_df.columns[:-1],outputCol=\"features\")\n",
    "    spark_assembler_df = assembler.transform(spark_df)\n",
    "    return spark_assembler_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_7_2_objective2_model(lrModel,test_data):\n",
    "    predictions = lrModel.transform(test_data)\n",
    "\n",
    "    pred_values = [ i.prediction for i in predictions.select('prediction').collect()]\n",
    "    \n",
    "    return predictions,pred_values,pred_values[0] ,np.sum(pred_values[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_8_2_percentage_viz(percentage_list, columns):\n",
    "    fig = plt.figure(figsize=(9, 5.0625))\n",
    "    ax1 = fig.add_subplot(121)\n",
    "\n",
    "    ratios = percentage_list\n",
    "    labels = columns\n",
    "    # rotate so that first wedge is split by the x-axis\n",
    "    angle = -180 * ratios[0]\n",
    "    ax1.pie(ratios, autopct='%1.1f%%', startangle=angle, labels=labels, )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_8_2_regression_line(data):\n",
    "    sns.regplot(x=\"real_value\", y=\"pred_value\", data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_8_2_residuals(data):\n",
    "    sns.residplot(x=\"real_value\", y=\"pred_value\", data=data, scatter_kws={\"s\": 80})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
